{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese_net.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ViOUwjGw0F9J",
        "VLBE6cMCxue1",
        "E1V9WwMJx1nR",
        "bU-wh6bLyAIx",
        "S4h6bEVPy38Y",
        "QGeVCOLUyTze",
        "vVR-19n6yW99",
        "6j-Rhs1kygH9",
        "Eq5ord0SLvza",
        "s9P7xbx7QZHO",
        "7OnQWTTdACSb",
        "edkAQ8sYXlTN",
        "Gl9oebZ1IG-o",
        "rRMK_ZpKIrH5",
        "xg4MpDx_zKBT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViOUwjGw0F9J",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# **SiameseNet for Recognizing Faces in the Wild**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9teeNEFC7Ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install git+https://github.com/rcmalli/keras-vggface.git\n",
        "\n",
        "!unzip drive/My\\ Drive/Proyecto/data.zip -d content/\n",
        "!unzip drive/My\\ Drive/Proyecto/test.zip -d content/test/\n",
        "!unzip drive/My\\ Drive/Proyecto/sample_submission.zip -d content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLBE6cMCxue1",
        "colab_type": "text"
      },
      "source": [
        "# **Imports:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr02EA6djY8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import image\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from keras.layers import Input, Dense, Dropout, Lambda\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from IPython.display import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1V9WwMJx1nR",
        "colab_type": "text"
      },
      "source": [
        "# **Read Data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l46O9GzwV86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_image(path):\n",
        "    \"\"\"\n",
        "    Funcion que permite leer imagenes a partir de un archivo\n",
        "    Args:\n",
        "        path: Ruta de la imagen\n",
        "    \"\"\"\n",
        "    img = cv2.imread(path)\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return preprocess_input(img, version=2)\n",
        "\n",
        "\n",
        "def read_family_members_images(data_path):\n",
        "    \"\"\"\n",
        "    Funcion que procesa la ruta especificada como parametro. Obtiene una lista\n",
        "    con los miembros de cada familia, la cual tendra el formato \"FXXXX/MIDY\",\n",
        "    y un diccionario con las rutas de las imagenes de cada miembro de cada familia.\n",
        "\n",
        "    Args:\n",
        "        data_path: Ruta de los archivos a procesar.\n",
        "    \n",
        "    Return:\n",
        "        Devuelve una lista con los miembros de cada familia y un diccionario con\n",
        "        las imagenes de cada miembro de cada familia.\n",
        "    \"\"\"\n",
        "    # Leer la ruta proporcionada y obtener todos los directorios\n",
        "    # Cada directorio esta asociado a una familia\n",
        "    dirs = sorted(list(glob.glob(data_path + \"*\")))\n",
        "\n",
        "    # Obtener los nombres de los directorios de las familias\n",
        "    family_dirs = np.array([dir.split(\"/\")[-1] for dir in dirs])\n",
        "\n",
        "    # Obtener imagenes asociadas a cada directorio\n",
        "    images = {f\"{family}/{member.split('/')[-1]}\": sorted(list(glob.glob(member + \"/*.jpg\")))\n",
        "        for family in family_dirs for member in sorted(list(glob.glob(f\"{data_path}{family}/*\")))\n",
        "    }\n",
        "    \n",
        "    family_members_list = list(images.keys())\n",
        "\n",
        "    return family_members_list, images\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU-wh6bLyAIx",
        "colab_type": "text"
      },
      "source": [
        "# **Data Generators:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOkK9EQOxWly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_datasets(families, val_prop=0.2):\n",
        "    \"\"\"\n",
        "    Funcion que permite generar los datasets de train, test y validacion\n",
        "    a partir de un array de directorios, los cuales representan las familias.\n",
        "    Los datos son mezclados para que se escoja de forma aleatoria.\n",
        "\n",
        "    Args:\n",
        "        families: Array con los directorios de las familias.\n",
        "        test_prop: Proporcion de los datos totales que tienen que estar en el\n",
        "                   conjunto de test.\n",
        "        val_prop: Proporcion de los (datos_totales - datos_test) que tienen que\n",
        "                  estar en el conjunto de validacion.\n",
        "    \n",
        "    Return:\n",
        "        Devuelve un array con los directorios de las familias que forman el conjunto\n",
        "        de train, otro para el conjunto de validacion y otro para el conjunto de test.\n",
        "    \"\"\"\n",
        "    # Mezclar familias\n",
        "    shuffle_families = np.copy(families)\n",
        "    np.random.shuffle(shuffle_families)\n",
        "\n",
        "    # Obtener la ultima proporcion de las familias y guardarla en el conjunto\n",
        "    # de test\n",
        "    idx_val = int(len(shuffle_families) * (1 - val_prop))\n",
        "    val_dirs = shuffle_families[idx_val:]\n",
        "    train_dirs = shuffle_families[:idx_val]\n",
        "\n",
        "    # Volver a mezclar familias para escoger conjunto de validacion\n",
        "    np.random.shuffle(train_dirs)\n",
        "\n",
        "    print(train_dirs.shape)\n",
        "    print(val_dirs.shape)\n",
        "\n",
        "    return train_dirs, val_dirs\n",
        "\n",
        "\n",
        "\n",
        "def dataset_to_images(dataset, images, relationships, size, relationships_prop):\n",
        "    \"\"\"\n",
        "    Funcion que genera dos arrays de individuos con un tama침o determinado, y otro que\n",
        "    nos indica el parentesco entre un par de individuos de cada uno de los arrays anteriores.\n",
        "    Los datos se escogeran de forma aleatoria entre todos los individuos proporcionados.\n",
        "    Args:\n",
        "        dataset: Array con los directorios de las familias.\n",
        "        images: Array con los directorios de las imagenes de cada individuo de la familia.\n",
        "        relationships: Relaciones entre los individuos a procesar.\n",
        "        size: Tama침o de los datos a generar.\n",
        "        relationships_prop: Proporcion de individuos con un parentesco familiar que tendran\n",
        "                            los datos generados.\n",
        "    \n",
        "    Return:\n",
        "        Devuelve dos arrays con los individuos que ser치n procesados por cada una de las\n",
        "        partes de nuestra red, y un array con los parentescos entre los arrays anteriores\n",
        "    \"\"\"    \n",
        "    left_images = []\n",
        "    right_images = []\n",
        "    targets = []\n",
        "\n",
        "    # Elegir los 1's\n",
        "    while len(left_images) < int(size*relationships_prop):\n",
        "        # Escogemos una linea aleatoria del CSV\n",
        "        index = np.random.choice(len(relationships))\n",
        "        ind = relationships[index]\n",
        "\n",
        "        # Comprobamos que los individuos estan en el dataset\n",
        "        if ind[0] in dataset and ind[1] in dataset:\n",
        "            # Elegimos aleatoriamente una imagen de esos individuos\n",
        "            left_images.append(read_image(np.random.choice( images[ind[0]] )))\n",
        "            right_images.append(read_image(np.random.choice( images[ind[1]] )))\n",
        "            targets.append(1.)\n",
        "\n",
        "    # Elegir los 0's\n",
        "    while len(left_images) < int(size):\n",
        "        # Accedemos dos individuos diferentes aleatorios del dataset\n",
        "        ind = np.random.choice(dataset, 2, replace=False)\n",
        "\n",
        "        # Comprobamos si son parientes\n",
        "        if (ind[0],ind[1]) not in relationships and (ind[1],ind[0]) not in relationships:\n",
        "            # En caso afirmativo a침adimos con etiqueta 1\n",
        "            left_images.append( read_image( np.random.choice(images[ind[0]]) ) )\n",
        "            right_images.append( read_image( np.random.choice(images[ind[1]]) ) )\n",
        "            targets.append(0.0)\n",
        "    \n",
        "    left_images = np.array(left_images)\n",
        "    right_images = np.array(right_images)\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    idx_perm = np.random.permutation(size)\n",
        "\n",
        "    left_images = left_images[idx_perm]\n",
        "    right_images = right_images[idx_perm]\n",
        "    targets = targets[idx_perm]\n",
        "\n",
        "    return left_images, right_images, targets\n",
        "\n",
        "\n",
        "\n",
        "def batch_generator(dataset, images, relationships_path, batch_size=32, relationships_prop=0.2):\n",
        "    \"\"\"\n",
        "    Funcion que selecciona aleatoriamente dos conjunto de individuos\n",
        "    y sus etiquetas, asigna una proporcion de parejas con parentesco\n",
        "    entre los dos conjuntos, y devuelve en cada iteracion la\n",
        "    cantidad asignada como tama침o de batch.\n",
        "    Args:\n",
        "        dataset: Array con los directorios de las familias.\n",
        "        images: Array con los directorios de las imagenes de cada individuo\n",
        "                de la familia.\n",
        "        relationships_path: Ruta del archivo de relaciones a procesar.\n",
        "        batch_size: Tama침o del batch.\n",
        "        relationships_prop: Proporcion de individuos con un parentesco familiar\n",
        "                            que tendran los datos generados.\n",
        "    \n",
        "    Return:\n",
        "        Devuelve dos arrays con los individuos que ser치n procesados por cada una\n",
        "        de las partes de nuestra red, y un array con los parentescos entre los\n",
        "        arrays anteriores\n",
        "    \"\"\"\n",
        "    # Leemos el archivo donde se encuentran las relaciones familiares entre individuos\n",
        "    relationships = pd.read_csv(relationships_path)\n",
        "    relationships = list(zip(relationships.p1.values, relationships.p2.values))\n",
        "    \n",
        "\n",
        "    while True:\n",
        "        # Generamos un conjunto de imagenes aleatorias y lo devolvemos hasta\n",
        "        # que el iterador vuelva a pedir otro\n",
        "        left_images, right_images, targets = dataset_to_images(dataset, images, relationships, batch_size, relationships_prop)\n",
        "\n",
        "        yield [left_images, right_images], targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4h6bEVPy38Y",
        "colab_type": "text"
      },
      "source": [
        "# **Graphics Generator:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8yLTeFXBZH_p",
        "colab": {}
      },
      "source": [
        "def show_history(hist):\n",
        "    \"\"\"\n",
        "    Funcion que muestra dos graficas con la evolucion de la funcion de perdida\n",
        "    y de la precision, respectivamente, obtenidas al entrenar nuestro modelo\n",
        "\n",
        "    Args:\n",
        "        hist: Registro de valores de entrenamiento y de validacion (si\n",
        "              corresponde) en 칠pocas sucesivas.\n",
        "    \"\"\"\n",
        "    loss = hist.history['loss']\n",
        "    val_loss = hist.history['val_loss']\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.legend(['Training loss', 'Validation loss'])\n",
        "    plt.show()\n",
        "\n",
        "    acc = hist.history['acc']\n",
        "    val_acc = hist.history['val_acc']\n",
        "    plt.plot(acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.legend(['Training accuracy', 'Validation accuracy'])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_model(model):\n",
        "    \"\"\"\n",
        "    Funcion que muestra la arquitectura de un modelo\n",
        "\n",
        "    Args:\n",
        "        model: Modelo a mostrar\n",
        "    \"\"\"\n",
        "    img = plot_model(model, show_shapes=True, show_layer_names=True)\n",
        "    display(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGeVCOLUyTze",
        "colab_type": "text"
      },
      "source": [
        "# **Initializers implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV4DMlzjRphY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_bias(shape, dtype=None):\n",
        "    \"\"\"\n",
        "    The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "    suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
        "    \"\"\"\n",
        "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)\n",
        "  \n",
        "def initialize_weights(shape, dtype=None):\n",
        "    \"\"\"\n",
        "    The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "    suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
        "    \"\"\"\n",
        "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVR-19n6yW99",
        "colab_type": "text"
      },
      "source": [
        "# **Binary Focal Loss implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQcUA7Onw-PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\n",
        "    Binary form of focal loss.\n",
        "\n",
        "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
        "\n",
        "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
        "\n",
        "    References:\n",
        "        https://arxiv.org/pdf/1708.02002.pdf\n",
        "    Usage:\n",
        "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
        "\n",
        "    \"\"\"\n",
        "    def binary_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred:  A tensor resulting from a sigmoid\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        epsilon = K.epsilon()\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
        "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
        "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
        "\n",
        "    return binary_focal_loss_fixed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j-Rhs1kygH9",
        "colab_type": "text"
      },
      "source": [
        "# **SiameseNet implementation:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwFQ6c84jbfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_siamesenet(model, optimizer, loss_function):\n",
        "    \"\"\"\n",
        "    Funcion que crea una red siamesa a partir de una serie de redes preentrenada\n",
        "    con el conjunto de datos VGGFace2. Este modelo declara dos entradas y,\n",
        "    posteriormente se conectan cada una de las partes mediante la distancia L1.\n",
        "    \n",
        "    Args:\n",
        "        model: Indica cual de las tres redes ya entrenadas con VGGFace2\n",
        "               utilizaremos en nuestro modelo: VGG16, RESNET50 o SENET50\n",
        "        optimizer: Indica cual sera el optimizador de nuestro modelo. Se puede\n",
        "                   crear una instancia antes de pasarlo, o puede llamarlo\n",
        "                   directamente por su nombre.\n",
        "        loss_function: Indica la funcion de perdida de nuestro modelo. Se puede\n",
        "                       pasar el nombre de una funcion de perdida existente o\n",
        "                       pasar una funcion simbolica.\n",
        "    \n",
        "    Return:\n",
        "        Devuelve la red ya creada y muestra un resumen de esta\n",
        "    \"\"\"\n",
        "    # Dimension de los datos de entrada\n",
        "    shape = (224, 224, 3)\n",
        "\n",
        "    # Declaramos 2 entradas, una para cada imagen\n",
        "    left_input = Input(shape)\n",
        "    right_input = Input(shape)\n",
        "\n",
        "    # Generamos nuestro modelo entrenado con VGGFace\n",
        "    vgg_model = VGGFace(model=model, include_top=False, weights=\"vggface\", pooling=\"max\")\n",
        "\n",
        "    # Conectar las partes de la red a cada una de las entradas\n",
        "    encoded_l = vgg_model(left_input)\n",
        "    encoded_r = vgg_model(right_input)\n",
        "\n",
        "    # Obtenemos la distancia L1 entre los dos tensores\n",
        "    L1_layer = Lambda(lambda tensor:K.abs(tensor[0] - tensor[1]))\n",
        "\n",
        "    # Aniadimos la funcion de distancia y la 칰ltima capa sigmoidal a la red\n",
        "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
        "\n",
        "    # Creamos el modelo\n",
        "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
        "    # Compilamos con un optimizador y una funcion de perdida determinadas\n",
        "    siamese_net.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
        "    # Mostramos un resumen de la red\n",
        "    siamese_net.summary()\n",
        "\n",
        "    return siamese_net\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq5ord0SLvza",
        "colab_type": "text"
      },
      "source": [
        "# **Dense SiameseNet implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QW2AVuf8LqTt",
        "colab": {}
      },
      "source": [
        "def create_dense_siamesenet(model, optimizer, loss_function):\n",
        "    \"\"\"\n",
        "    Funcion que crea una red siamesa a partir de una serie de redes preentrenada\n",
        "    con el conjunto de datos VGGFace2. Este modelo declara dos entradas y,\n",
        "    posteriormente se conectan cada una de las partes mediante la distancia L1.\n",
        "    Finalmente se la aniaden 2 capas FC para aplicar con los pesos obtenidos.\n",
        "    \n",
        "    Args:\n",
        "        model: Indica cual de las tres redes ya entrenadas con VGGFace2\n",
        "               utilizaremos en nuestro modelo: VGG16, RESNET50 o SENET50\n",
        "        optimizer: Indica cual sera el optimizador de nuestro modelo. Se puede\n",
        "                   crear una instancia antes de pasarlo, o puede llamarlo\n",
        "                   directamente por su nombre.\n",
        "        loss_function: Indica la funcion de perdida de nuestro modelo. Se puede\n",
        "                       pasar el nombre de una funcion de perdida existente o\n",
        "                       pasar una funcion simbolica.\n",
        "    \n",
        "    Return:\n",
        "        Devuelve la red ya creada y muestra un resumen de esta\n",
        "    \"\"\"\n",
        "    # Dimension de los datos de entrada\n",
        "    shape = (224, 224, 3)\n",
        "\n",
        "    # Declaramos 2 entradas, una para cada imagen\n",
        "    left_input = Input(shape)\n",
        "    right_input = Input(shape)\n",
        "\n",
        "    # Generamos nuestro modelo entrenado con VGGFace\n",
        "    vgg_model = VGGFace(model=model, include_top=False, weights=\"vggface\", pooling=\"max\")\n",
        "\n",
        "    # Conectar las partes de la red a cada una de las entradas\n",
        "    encoded_l = vgg_model(left_input)\n",
        "    encoded_r = vgg_model(right_input)\n",
        "\n",
        "    # Obtenemos la distancia L1 entre los dos tensores\n",
        "    L1_layer = Lambda(lambda tensor:K.abs(tensor[0] - tensor[1]))\n",
        "\n",
        "    # Aniadimos la funcion de distancia\n",
        "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "\n",
        "    # Aniadimos 2 capas FC y otras 2 de Dropout tras cada una\n",
        "    x = Dense(100, activation=\"relu\")(L1_distance)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(25, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    # Aniadimos la ultima capa sigmoidal a la red\n",
        "    prediction = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Creamos el modelo\n",
        "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
        "    # Compilamos con un optimizador y una funcion de perdida determinadas\n",
        "    siamese_net.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
        "    # Mostramos un resumen de la red\n",
        "    siamese_net.summary()\n",
        "\n",
        "    return siamese_net\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s9P7xbx7QZHO"
      },
      "source": [
        "# **Regularized SiameseNet implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6VHOFFd2QZHR",
        "colab": {}
      },
      "source": [
        "def create_regularized_siamesenet(model, optimizer, loss_function):\n",
        "    \"\"\"\n",
        "    Funcion que crea una red siamesa a partir de una serie de redes preentrenada\n",
        "    con el conjunto de datos VGGFace2. Este modelo declara dos entradas y,\n",
        "    posteriormente se conectan cada una de las partes mediante la distancia L1.\n",
        "    Finalmente se la aniaden 2 capas FC inicializando los pesos y el bias, y\n",
        "    con una funcion de regularizacion l2.\n",
        "    \n",
        "    Args:\n",
        "        model: Indica cual de las tres redes ya entrenadas con VGGFace2\n",
        "               utilizaremos en nuestro modelo: VGG16, RESNET50 o SENET50\n",
        "        optimizer: Indica cual sera el optimizador de nuestro modelo. Se puede\n",
        "                   crear una instancia antes de pasarlo, o puede llamarlo\n",
        "                   directamente por su nombre.\n",
        "        loss_function: Indica la funcion de perdida de nuestro modelo. Se puede\n",
        "                       pasar el nombre de una funcion de perdida existente o\n",
        "                       pasar una funcion simbolica.\n",
        "    \n",
        "    Return:\n",
        "        Devuelve la red ya creada y muestra un resumen de esta\n",
        "    \"\"\"\n",
        "    # Dimension de los datos de entrada\n",
        "    shape = (224, 224, 3)\n",
        "\n",
        "    # Declaramos 2 entradas, una para cada imagen\n",
        "    left_input = Input(shape)\n",
        "    right_input = Input(shape)\n",
        "\n",
        "    # Generamos nuestro modelo entrenado con VGGFace\n",
        "    vgg_model = VGGFace(model=model, include_top=False, weights=\"vggface\", pooling=\"max\")\n",
        "\n",
        "    # Conectar las partes de la red a cada una de las entradas\n",
        "    encoded_l = vgg_model(left_input)\n",
        "    encoded_r = vgg_model(right_input)\n",
        "\n",
        "    # Obtenemos la distancia L1 entre los dos tensores\n",
        "    L1_layer = Lambda(lambda tensor:K.abs(tensor[0] - tensor[1]))\n",
        "\n",
        "    # Aniadimos la funcion de distancia\n",
        "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "\n",
        "    # Aniadimos 2 capas FC inicializadas y regularizadas,\n",
        "    # y otras 2 de Dropout tras cada una\n",
        "    x = Dense(100, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-3),\n",
        "                                      kernel_initializer=initialize_weights,\n",
        "                                      bias_initializer=initialize_bias)(L1_distance)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(25, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-3),\n",
        "                                     kernel_initializer=initialize_weights,\n",
        "                                     bias_initializer=initialize_bias)(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    # Aniadimos la ultima capa sigmoidal a la red\n",
        "    prediction = Dense(1, activation='sigmoid', bias_initializer=initialize_bias)(x)\n",
        "\n",
        "    # Creamos el modelo\n",
        "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
        "    # Compilamos con un optimizador y una funcion de perdida determinadas\n",
        "    siamese_net.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
        "    # Mostramos un resumen de la red\n",
        "    siamese_net.summary()\n",
        "\n",
        "    return siamese_net\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4LcPm1yyvCn",
        "colab_type": "text"
      },
      "source": [
        "# **Main**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OnQWTTdACSb",
        "colab_type": "text"
      },
      "source": [
        "## Reading and preprocessing data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCfIAE8QjlEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Establecer semilla\n",
        "np.random.seed(1)\n",
        "\n",
        "# Establecer si se van a entrenar todas las redes\n",
        "train_all_nets = False\n",
        "\n",
        "# Lectura y preprocesado de los datos\n",
        "train_folders_path = \"content/train/\"\n",
        "train_relationships = \"content/train_relationships.csv\"\n",
        "\n",
        "dirs, images = read_family_members_images(train_folders_path)\n",
        "train_dirs, val_dirs = generate_datasets(dirs)\n",
        "print(train_dirs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edkAQ8sYXlTN",
        "colab_type": "text"
      },
      "source": [
        "## Train All Tested Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax0OL0RWXsL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entrenar todas las redes probadas si se ha especificado\n",
        "if train_all_nets:\n",
        "    # Establecer optimizadores que se utilizaran\n",
        "    adam = Adam()\n",
        "    adam_mod = Adam(0.00001)\n",
        "    rmsprop = RMSprop()\n",
        "\n",
        "    # Establecer todas las funciones de perdida que se utilizaran\n",
        "    binary_loss = \"binary_crossentropy\"\n",
        "    focal_loss = [binary_focal_loss(alpha=.25, gamma=2)]\n",
        "\n",
        "    # Crear todos los modelos probados (menos el definitivo)\n",
        "    tested_models = [\n",
        "        create_siamesenet('vgg16', adam, binary_loss),\n",
        "        create_siamesenet('resnet50', adam, binary_loss),\n",
        "        create_siamesenet('senet50', adam, binary_loss),\n",
        "        create_siamesenet('resnet50', adam_mod, binary_loss),\n",
        "        create_dense_siamesenet('resnet50', adam_mod, binary_loss),\n",
        "        create_dense_siamesenet('resnet50', rmsprop, binary_loss),\n",
        "        create_dense_siamesenet('resnet50', adam_mod, focal_loss)\n",
        "    ]\n",
        "\n",
        "    # Entrenar cada uno de los modelos y mostrar historia\n",
        "    for model in tested_models:\n",
        "        hist = model.fit_generator(batch_generator(train_dirs, images, train_relationships, batch_size=32, relationships_prop=0.6),\n",
        "                    validation_data=batch_generator(val_dirs, images, train_relationships, batch_size=32, relationships_prop=0.6),\n",
        "                    epochs=30, verbose=1, steps_per_epoch=100, validation_steps=30)\n",
        "\n",
        "        show_history(hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl9oebZ1IG-o",
        "colab_type": "text"
      },
      "source": [
        "## Creating Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKD2olTpl4cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instanciamos optimizador y funcion de perdida\n",
        "optimizer = Adam(0.00001)\n",
        "loss_function = \"binary_crossentropy\"\n",
        "\n",
        "# Creamos la red\n",
        "model = create_regularized_siamesenet('resnet50', optimizer, loss_function)\n",
        "\n",
        "# Imprimimos la arquitectura de la red\n",
        "visualize_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRMK_ZpKIrH5",
        "colab_type": "text"
      },
      "source": [
        "## Fitting Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgDPCaVqmriT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = model.fit_generator(batch_generator(train_dirs, images, train_relationships, batch_size=32, relationships_prop=0.6),\n",
        "                    validation_data=batch_generator(val_dirs, images, train_relationships, batch_size=32, relationships_prop=0.6),\n",
        "                    epochs=30, verbose=1, steps_per_epoch=100, validation_steps=30)\n",
        "\n",
        "show_history(hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg4MpDx_zKBT",
        "colab_type": "text"
      },
      "source": [
        "# **Test Data Generator:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWRd3QdhcVcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chunker(seq, size=32):\n",
        "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "test_path = \"content/test/\"\n",
        "submission = pd.read_csv(\"content/sample_submission.csv\")\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for batch in tqdm(chunker(submission.img_pair.values)):\n",
        "    X1 = [x.split(\"-\")[0] for x in batch]\n",
        "    X1 = np.array([read_image(test_path + x) for x in X1])\n",
        "\n",
        "    X2 = [x.split(\"-\")[1] for x in batch]\n",
        "    X2 = np.array([read_image(test_path + x) for x in X2])\n",
        "\n",
        "    pred = model.predict([X1, X2]).ravel().tolist()\n",
        "    predictions += pred\n",
        "\n",
        "submission['is_related'] = predictions\n",
        "submission.to_csv(\"drive/My Drive/Proyecto/siamese_net.csv\", index=False)\n",
        "print(submission)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}